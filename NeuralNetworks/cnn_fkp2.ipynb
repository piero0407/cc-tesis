{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from fkp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DMutil import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPropagation(X, W, b, G):\n",
    "    L = len(W)\n",
    "    A = X\n",
    "    for l in range(1, L):\n",
    "        Z = tf.matmul(A, W[l]) + b[l]\n",
    "        A = G[l](Z)\n",
    "        \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeParameters(layers):\n",
    "    L = len(layers)\n",
    "    W = [None]*L\n",
    "    b = [None]*L\n",
    "    for l in range(1, L):\n",
    "        W[l] = weightVariable([layers[l-1], layers[l]])\n",
    "        b[l] = biasVariable([1, layers[l]])\n",
    "        \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnnModel(tra, val, #tes,\n",
    "             convLayers,\n",
    "             layers,\n",
    "             activations,\n",
    "             numIter=10,\n",
    "             batchSize=128,\n",
    "             alpha=0.5,\n",
    "             lambd=0.01,\n",
    "             beta1=0.9,\n",
    "             beta2=0.999,\n",
    "             epsilon=1e-08,\n",
    "             printLoss=True,\n",
    "             numPrints=10):\n",
    "    interval = int(numIter / numPrints)\n",
    "    m = tra['X'].shape[0]\n",
    "    L = len(layers)\n",
    "    \n",
    "    nextBatch = lambda data: data[offset:offset+batchSize]\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        np.random.seed(1981)\n",
    "        tf.set_random_seed(1981)\n",
    "        X = tf.placeholder(tf.float32, shape=[None, tra['X'].shape[1]])\n",
    "        Y = tf.placeholder(tf.float32, shape=[None, tra['Y'].shape[1]])\n",
    "        \n",
    "        fil = 96\n",
    "        col = 96\n",
    "        Ximg = tf.reshape(X, [-1 , fil, col, 1])\n",
    "        CL = len(convLayers)\n",
    "        WK = [None]*CL\n",
    "        bK = [None]*CL\n",
    "        for l in range(1, CL):\n",
    "            WK[l] = weightVariable([convLayers[l][0], convLayers[l][0], convLayers[l-1][1], convLayers[l][1]])\n",
    "            bK[l] = biasVariable([1, convLayers[l][1]])\n",
    "        \n",
    "        for l in range(1, CL):\n",
    "            h_conv = tf.nn.relu(tf.nn.conv2d(h_conv, WK[l], strides=[1, 1, 1, 1], padding='SAME') + bK[l])\n",
    "            h_pool = tf.nn.max_pool(h_conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            fil = fil // 2\n",
    "            col = col // 2\n",
    "        \n",
    "        h_flat = tf.reshape(h_conv, [-1, fil*col*convLayer[CL-1][1]])\n",
    "        \n",
    "        W, b = initializeParameters(layers)\n",
    "        \n",
    "        Y_ = forwardPropagation(h_flat, W, b, activations)\n",
    "        \n",
    "        # Reg L2\n",
    "        #regularizer = tf.nn.l2_loss(W[L-1])\n",
    "        \n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=Y_)\n",
    "#         loss = tf.reduce_mean(loss + lambd * regularizer)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        step = tf.train.GradientDescentOptimizer(alpha).minimize(loss)\n",
    "#         step = tf.train.AdamOptimizer(learning_rate=alpha,\n",
    "#                                       beta1=beta1,\n",
    "#                                       beta2=beta2,\n",
    "#                                       epsilon=epsilon).minimize(loss)\n",
    "        \n",
    "        prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(numIter):\n",
    "            p = np.random.permutation(m)\n",
    "            tra['X'], tra['Y'] = tra['X'][p], tra['Y'][p]\n",
    "            for offset in range(0, m, batchSize):\n",
    "                fd = { X: nextBatch(tra['X']), Y: nextBatch(tra['Y']) }\n",
    "                sess.run(step, feed_dict=fd)\n",
    "            if printLoss and (epoch + 1) % interval == 0:\n",
    "                print('Loss at %4d: %12.6f'%(epoch, sess.run(loss, feed_dict=fd)))\n",
    "                \n",
    "        prctAcc = sess.run(accuracy, feed_dict={X: tra['X'], Y: tra['Y']})\n",
    "        print(\"Training accuracy: %6.2f%%\"%(prctAcc*100))\n",
    "        prctAcc = sess.run(accuracy, feed_dict={X: val['X'], Y: val['Y']})\n",
    "        print(\"Validation accuracy: %6.2f%%\"%(prctAcc*100))\n",
    "#         prctAcc = sess.run(accuracy, feed_dict={X: tes['X'], Y: tes['Y']})\n",
    "#         print(\"Test accuracy: %6.2f%%\"%(prctAcc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dnnModel() missing 1 required positional argument: 'activations'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-5bc3eeb261ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m               numPrints=10)\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-5bc3eeb261ff>\u001b[0m in \u001b[0;36mtest\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m               \u001b[0mbatchSize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtra\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m               \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m               numPrints=10)\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: dnnModel() missing 1 required positional argument: 'activations'"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    tra, val = loadMNIST()\n",
    "    activations = [None,\n",
    "                   lambda z: tf.nn.relu(z),\n",
    "                   lambda z: tf.nn.relu(z),\n",
    "                   lambda z: z]\n",
    "    convLayers = [[1, 1], [3, 8], [3, 16]],\n",
    "    layers = [24*24*16, 16, 8, 30]\n",
    "    dnnModel(tra, val,\n",
    "              layers,\n",
    "              activations,\n",
    "              numIter=10,\n",
    "              batchSize=tra['X'].shape[0],\n",
    "              alpha=0.01,\n",
    "              numPrints=10)\n",
    "        \n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
